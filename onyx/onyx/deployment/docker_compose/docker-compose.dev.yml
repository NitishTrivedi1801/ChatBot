services:
  web_server:
    build:
      context: ../../web
      dockerfile: Dockerfile
      args:
        INTERNAL_URL: http://api_server:8080
    image: onyxdotapp/onyx-web-server:latest
    environment:
      - INTERNAL_URL=http://api_server:8080
      - NEXT_PUBLIC_API_BASE_URL=http://api_server:8080/api
      - GEN_AI_API_KEY=${GEN_AI_API_KEY}
      - USE_LLAMA_FALLBACK=${USE_LLAMA_FALLBACK}
    container_name: docker_compose-web_server-1
    ports:
      - "3000:3000"  # Expose internal port 3000 to nginx
    depends_on:
      - api_server
    restart: always
    networks:
      - onyx

  api_migrations:
    build:
      context: ../..
      dockerfile: backend/Dockerfile.api_migrations
    command: ["alembic", "upgrade", "head"]
    depends_on:
      - relational_db
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
      - POSTGRES_HOST=relational_db
      - POSTGRES_PORT=5432
      - POSTGRES_DEFAULT_SCHEMA=public
      - DATABASE_URL=postgresql+asyncpg://postgres:password@relational_db:5432/postgres
    volumes:
      - /Users/pranjal.khatod/onyx/backend:/backend
    networks:
      - onyx
  api_server:
    image: onydotapp/onyx-backend-api:latest
    build:
      context: ../..
      dockerfile: backend/Dockerfile
    command: ["bash", "-c", "supervisord -c /etc/supervisor/conf.d/supervisord.conf"]
    environment:
      # Postgres
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
      - POSTGRES_HOST=relational_db
      - POSTGRES_PORT=5432
      - POSTGRES_DEFAULT_SCHEMA=public
      - DATABASE_URL=postgresql+asyncpg://postgres:password@relational_db:5432/postgres

      # Redis
      - REDIS_HOST=cache

      # Vespa
      - VESPA_HOST=index
      - VESPA_PORT=8081
      - VESPA_CONFIG_SERVER_HOST=index
      - VESPA_TENANT_PORT=19071

      # Embeddings
      - EMBEDDING_PROVIDER=openai
      - EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}

      # Gen AI
      - GEN_AI_API_KEY= # Leave empty for llama fallback
      - GEN_AI_INFERENCE_SERVER=http://inference_model_server:8001 # Ensure correct port for inference
      - GEN_AI_MODEL_PROVIDER=openai
      - GEN_AI_MODEL_VERSION=gpt-3.5-turbo
      - FAST_GEN_AI_MODEL_VERSION=gpt-3.5-turbo
      - USE_LLAMA_FALLBACK=true # Enable llama fallback for local dev

      # Auth SSO
      - AUTH_TYPE=${AUTH_TYPE}
      - GOOGLE_OAUTH_CLIENT_ID=${GOOGLE_OAUTH_CLIENT_ID}
      - GOOGLE_OAUTH_CLIENT_SECRET=${GOOGLE_OAUTH_CLIENT_SECRET}
      - VALID_EMAIL_DOMAINS=${VALID_EMAIL_DOMAINS}
      - MODEL_SERVER_HOST=inference_model_server
      - MODEL_SERVER_PORT=8001
      - INDEXING_MODEL_SERVER_HOST=indexing_model_server
      - INDEXING_MODEL_SERVER_PORT=8002
      
    container_name: docker_compose-api_server-1
    ports:
      - "8080:8080"
    restart: always
    depends_on:
      - api_migrations
      - relational_db
      - inference_model_server # Ensure api_server waits for inference model server
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/health"]
      interval: 20s
      timeout: 5s
      retries: 5
    networks:
      - onyx

  background:
    image: onyxdotapp/onyx-backend-bg:latest
    build:
      context: ../..
      dockerfile: backend/Dockerfile
    command: [
      "celery",
      "-A",
      "onyx.background.celery.versioned_apps.indexing",
      "worker",
      "--pool=threads",
      "--concurrency=1",
      "--prefetch-multiplier=1",
      "--loglevel=INFO",
      "--hostname=indexing@%n",
      "--queues=connector_indexing"
    ]
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
      - POSTGRES_HOST=relational_db
      - POSTGRES_PORT=5432
      - POSTGRES_DEFAULT_SCHEMA=public
      - DATABASE_URL=postgresql+asyncpg://postgres:password@relational_db:5432/postgres
      - MODEL_SERVER_HOST=inference_model_server
      - MODEL_SERVER_PORT=8001
      - REDIS_HOST=cache
      - VESPA_HOST=index
      - VESPA_PORT=8081
      - VESPA_CONFIG_SERVER_HOST=index
      - VESPA_TENANT_PORT=19071
    depends_on:
      - api_server
      - cache
    restart: always
    networks:
      - onyx
  background_primary:
    image: onyxdotapp/onyx-backend-bg:latest
    build:
      context: ../..
      dockerfile: backend/Dockerfile
    command:
      - "celery"
      - "-A"
      - "onyx.background.celery.versioned_apps.primary"
      - "worker"
      - "--pool=threads"
      - "--concurrency=4"
      - "--loglevel=INFO"
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
      - POSTGRES_HOST=relational_db
      - POSTGRES_PORT=5432
      - POSTGRES_DEFAULT_SCHEMA=public
      - DATABASE_URL=postgresql+asyncpg://postgres:password@relational_db:5432/postgres
      - MODEL_SERVER_HOST=inference_model_server
      - MODEL_SERVER_PORT=8001
      - REDIS_HOST=cache
      - VESPA_HOST=index
      - VESPA_PORT=8081
      - VESPA_CONFIG_SERVER_HOST=index
      - VESPA_TENANT_PORT=19071
      - DANSWER_BOT_DISABLE_DOCS_ONLY_ANSWER=${DANSWER_BOT_DISABLE_DOCS_ONLY_ANSWER:-}
      - DANSWER_BOT_FEEDBACK_VISIBILITY=${DANSWER_BOT_FEEDBACK_VISIBILITY:-}
      - DANSWER_BOT_DISPLAY_ERROR_MSGS=${DANSWER_BOT_DISPLAY_ERROR_MSGS:-}
      - DANSWER_BOT_RESPOND_EVERY_CHANNEL=${DANSWER_BOT_RESPOND_EVERY_CHANNEL:-}
      - DANSWER_BOT_DISABLE_COT=${DANSWER_BOT_DISABLE_COT:-}
      - NOTIFY_SLACKBOT_NO_ANSWER=${NOTIFY_SLACKBOT_NO_ANSWER:-}
      - DANSWER_BOT_MAX_QPM=${DANSWER_BOT_MAX_QPM:-}
      - DANSWER_BOT_MAX_WAIT_TIME=${DANSWER_BOT_MAX_WAIT_TIME:-}
    depends_on:
      - api_server
    restart: always
    networks:
      - onyx

  inference_model_server:
    image: onyxdotapp/onyx-model-server-inference:latest
    build:
      context: ../..
      dockerfile: backend/Dockerfile.model_server
    command: >
      /bin/sh -c "if [ \"${DISABLE_MODEL_SERVER:-false}\" = \"true\" ]; then echo 'Skipping service'; exit 0; fi &&
      exec /app/start_model_server.sh"
    environment:
    - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER}
    - EMBEDDING_MODEL_NAME=${EMBEDDING_MODEL_NAME}
    - MODEL_SERVER_PORT=8001
    volumes:
      - model_cache_huggingface:/root/.cache/huggingface
      - inference_model_server_logs:/var/log
    ports:
      - "8001:8001"
    restart: always
    healthcheck:
      test:
        [
          "CMD", "python3", "-c",
          "import requests; r = requests.post('http://localhost:8001/encoder/bi-encoder-embed', json={\"texts\": [\"healthcheck\"], \"max_context_length\": 512, \"normalize_embeddings\": False, \"text_type\": \"query\", \"model_name\": \"nomic-ai/nomic-embed-text-v1\"}); exit(0) if r.ok else exit(1)"
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - onyx
  indexing_model_server:
    image: onyxdotapp/onyx-model-server-indexing:latest
    build:
      context: ../..
      dockerfile: backend/Dockerfile.model_server
    command: >
      /bin/sh -c "if [ \"${DISABLE_MODEL_SERVER:-false}\" = \"true\" ]; then echo 'Skipping service'; exit 0; fi &&
      INDEXING_ONLY=True exec /app/start_model_server.sh"
    environment:
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER}
      - EMBEDDING_MODEL_NAME=${EMBEDDING_MODEL_NAME}
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      - MODEL_SERVER_PORT=8002
    volumes:
      - model_cache_huggingface:/root/.cache/huggingface
    ports:
      - "8002:8002"
    restart: always
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-s", "-X", "POST", "http://localhost:8002/encoder/bi-encoder-embed", 
             "-H", "Content-Type: application/json", 
             "-d", '{"texts": ["healthcheck"], "max_context_length": 512, "normalize_embeddings": false, "text_type": "query", "model_name": "nomic-ai/nomic-embed-text-v1"}']
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - onyx
  cache:
    image: redis:7.4-alpine
    ports:
      - "6379:6379"
    restart: always
    networks:
      - onyx
  relational_db:   
      image: postgres:15.2
      container_name: relational_db
      restart: unless-stopped
      ports:
        - "5433:5432"
      environment:
        POSTGRES_DB: postgres
        POSTGRES_USER: postgres
        POSTGRES_PASSWORD: password
      networks:
        - onyx
  index:
    image: vespaengine/vespa:8.526.15
    ports:
      - "8081:8081"
      - "19071:19071"
    restart: always
    networks:
      - onyx
  nginx:
    image: nginx:1.25
    depends_on:
      - web_server
      - api_server
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped
    networks:
      - onyx
  celery_primary:
    image: onyxdotapp/onyx-backend-bg:latest
    build:
      context: ../..
      dockerfile: backend/Dockerfile
    command: [
      "celery",
      "-A",
      "onyx.background.celery.versioned_apps.primary",
      "worker",
      "--pool=threads",
      "--concurrency=1",
      "--prefetch-multiplier=1",
      "--loglevel=INFO",
      "--hostname=primary@%n",
      "-Q",
      "celery"
    ]
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
      - POSTGRES_HOST=relational_db
      - POSTGRES_PORT=5432
      - POSTGRES_DEFAULT_SCHEMA=public
      - DATABASE_URL=postgresql+asyncpg://postgres:password@relational_db:5432/postgres
      - MODEL_SERVER_HOST=inference_model_server
      - MODEL_SERVER_PORT=8001
      - REDIS_HOST=cache
      - VESPA_HOST=index
      - VESPA_PORT=8081
      - VESPA_CONFIG_SERVER_HOST=index
      - VESPA_TENANT_PORT=19071
    depends_on:
      - api_server
    restart: always
    networks:
      - onyx

volumes:
  model_cache_huggingface:
  inference_model_server_logs:

networks:
  onyx:
    driver: bridge
